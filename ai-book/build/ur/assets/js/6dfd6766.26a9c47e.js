"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook=globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[154],{7637:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>r,default:()=>u,frontMatter:()=>a,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"vla-systems/module-4-vla-foundations","title":"Module 4: Vision-Language-Action (VLA) Systems - Foundations","description":"Empowering humanoids to perceive, understand, and interact with the world through multimodal AI","source":"@site/docs/05-vla-systems/module-4-vla-foundations.md","sourceDirName":"05-vla-systems","slug":"/vla-systems/module-4-vla-foundations","permalink":"/ur/docs/vla-systems/module-4-vla-foundations","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/05-vla-systems/module-4-vla-foundations.md","tags":[],"version":"current","frontMatter":{"title":"Module 4: Vision-Language-Action (VLA) Systems - Foundations","description":"Empowering humanoids to perceive, understand, and interact with the world through multimodal AI","module":4,"duration":"6-8 hours","prerequisites":"ROS 2, basic AI/ML concepts, Python","objectives":["Understand the architecture and components of VLA systems for robotics","Explore key AI models for visual perception, natural language understanding, and action generation","Integrate multimodal sensors (cameras, microphones) with VLA pipelines (conceptual)","Develop basic VLA behaviors for simulated humanoid robots (design only)","Grasp the ethical considerations and challenges in VLA development"]},"sidebar":"tutorialSidebar","previous":{"title":"Module 3: Hardware Foundations & Edge AI","permalink":"/ur/docs/hardware-basics/module-3-hardware"},"next":{"title":"VLA Action Systems","permalink":"/ur/docs/vla-systems/vla-action"}}');var o=t(4848),i=t(8453);const a={title:"Module 4: Vision-Language-Action (VLA) Systems - Foundations",description:"Empowering humanoids to perceive, understand, and interact with the world through multimodal AI",module:4,duration:"6-8 hours",prerequisites:"ROS 2, basic AI/ML concepts, Python",objectives:["Understand the architecture and components of VLA systems for robotics","Explore key AI models for visual perception, natural language understanding, and action generation","Integrate multimodal sensors (cameras, microphones) with VLA pipelines (conceptual)","Develop basic VLA behaviors for simulated humanoid robots (design only)","Grasp the ethical considerations and challenges in VLA development"]},r="Module 4: Vision-Language-Action (VLA) Systems - Foundations",d={},l=[{value:"Bridging Perception, Cognition, and Embodiment",id:"bridging-perception-cognition-and-embodiment",level:2},{value:"Learning Outcomes (static)",id:"learning-outcomes-static",level:2},{value:"Design Patterns &amp; Diagrams",id:"design-patterns--diagrams",level:2}];function c(e){const n={h1:"h1",h2:"h2",header:"header",hr:"hr",li:"li",p:"p",ul:"ul",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"module-4-vision-language-action-vla-systems---foundations",children:"Module 4: Vision-Language-Action (VLA) Systems - Foundations"})}),"\n",(0,o.jsx)(n.h2,{id:"bridging-perception-cognition-and-embodiment",children:"Bridging Perception, Cognition, and Embodiment"}),"\n",(0,o.jsx)(n.p,{children:"VLA systems enable robots to perceive, understand, and act. This module focuses on architecture, model choices and deployment patterns \u2014 presented as design patterns and non-executable examples."}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"learning-outcomes-static",children:"Learning Outcomes (static)"}),"\n",(0,o.jsx)(n.p,{children:"After this module, students will be able to:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Describe VLA system components and their interfaces."}),"\n",(0,o.jsx)(n.li,{children:"Create a multimodal pipeline diagram that shows how sensors \u2192 perception \u2192 LLM planner \u2192 action generator interact."}),"\n",(0,o.jsx)(n.li,{children:"List evaluation metrics for perception and actionable tasks."}),"\n",(0,o.jsx)(n.li,{children:"Discuss ethical and safety considerations in multimodal robotics."}),"\n"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"design-patterns--diagrams",children:"Design Patterns & Diagrams"}),"\n",(0,o.jsx)(n.p,{children:"Include conceptual diagrams for:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Multimodal fusion (vision + language features \u2192 planner)"}),"\n",(0,o.jsx)(n.li,{children:"Closed-loop perception-planning-action cycles"}),"\n",(0,o.jsx)(n.li,{children:"Safety monitors and human override channels"}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>r});var s=t(6540);const o={},i=s.createContext(o);function a(e){const n=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);